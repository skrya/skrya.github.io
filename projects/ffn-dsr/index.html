<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Forecasting Future Videos from Novel Views via Disentangled 3D Scene Representation.">
  <meta name="keywords" content="forecasting, future, novel view synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Forecasting Future Videos from Novel Views via Disentangled 3D Scene Representation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> -->

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Forecasting Future Videos from Novel Views via Disentangled 3D Scene Representation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://skrya.github.io/">Sudhir Yarram</a><sup>1</sup>,</span>
            <span class="author-block"> <a href="https://cse.buffalo.edu/~jsyuan/">
              Junsong Yuan</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University at Buffalo</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><strong>European Conference on Computer Vision (ECCV) 2024</strong></span>
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://skrya.github.io/projects/ffn-dsr/static/pdfs/09842.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.21450"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!--  Code Link. -->
              <span class="link-block">
                <a href="https://skrya.github.io/projects/ffn-dsr/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon!)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser" style="height: 100%; width: 100%;">
  <div class="container is-max-desktop" style="height: 100%; width: 100%;">
    <div class="hero-body" style="height: 100%; width: 100%; display: flex; justify-content: center; align-items: center;">
      <div class="column is-3 has-text-centered" style="height: 100%; width: 100%;">
        <img src="./static/images/final_cvpr_fig1.png"
             class="interpolation-image"
             alt="Interpolate start reference image."
             style="height: 100%; width: 100%; object-fit: cover;"/>
        <p>Comparisons of video extrapolation in space and time (VEST) approaches.</p>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video extrapolation in space and time (VEST) enables viewers to forecast a 3D scene into the future and view it from novel viewpoints. Recent methods propose to learn an entangled representation, aiming to model layered scene geometry, motion forecasting and novel view synthesis together, while assuming simplified affine motion and homography-based warping at each scene layer, leading to inaccurate video extrapolation. 
 Instead of entangled scene representation and rendering, our approach chooses to disentangle scene geometry from scene motion, via lifting the 2D scene to 3D point clouds, which enables high quality rendering of future videos from novel views. To model future 3D scene motion, we propose a disentangled two-stage approach that initially forecasts ego-motion and subsequently the residual motion of dynamic objects (e.g., cars, people). This approach ensures more precise motion predictions by reducing inaccuracies from entanglement of ego-motion with dynamic object motion, where better ego-motion forecasting could significantly enhance the visual outcomes. Extensive experimental analysis on two urban scene datasets demonstrate superior performance of our proposed method in comparison to strong baselines.  
          </p>
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div> -->
      <!-- <h2 class="title is-3">Video</h2>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/ECCV_9842_v2_compressed.mp4"
                type="video/mp4">
      </video>  -->
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="./static/images/final_cvpr_fig2.png"
             class="interpolation-image"
             alt="Interpolate start reference image."
             style="height: 100%; width: 100%; object-fit: cover;"/>
        </div>
        <div class="content has-text-justified">
          Our framework aims to forecast a 3D scene into the future and view it from novel viewpoints. It comprises three primary steps:
        </div>
        
        <div class="content has-text-justified bold">
          <strong>1. Constructing 3D point clouds:</strong>
        </div>
        
        <div class="content has-text-justified">
          Starting with two past frames as the input, we construct per-frame 3D point clouds.
          <br> 
          (i) The process for each frame involves depth estimation, dis-occlusion handling via inpainting, and feature extraction to finally generate what we refer to as feature layer.
          <br> 
          (ii) The point-wise features in this feature layer are then lifted into 3D space using corresponding depth values, generating 3D point clouds. This process is performed on both \( \mathbf{I}_{(t-1)} \) and \( \mathbf{I}_{(t)} \) to obtain feature layers \( \mathcal{F}_{(t-1)} \) and \( \mathcal{F}_{(t)} \) and point clouds \( \mathcal{P}_{(t-1)} \) and \( \mathcal{P}_{(t)} \).
        </div>
        
        <div class="content has-text-justified bold">
          <strong>2. Forecasting future 3D motion: </strong>
        </div>
        
        <div class="content has-text-justified">
          We leverage the feature layers \( \mathcal{F}_{(t-1)} \) and \( \mathcal{F}_{(t)} \) to forecast future 3D motion for each of the point clouds. This forecasted 3D motion allows us to update the positions of point clouds \( \mathcal{P}_{(t-1)} \) and \( \mathcal{P}_{(t)} \) to their new, forecasted locations.
        </div>

        
        <div class="content has-text-justified bold">
          <strong>3. Splatting and Rendering: </strong>
        </div>
        
        <div class="content has-text-justified">
          A point-based renderer processes these motion-adjusted point clouds through 3D-to-2D splatting to generate feature maps. Finally, a refinement network takes these rendered feature maps and decodes them to synthesize a novel view \( \hat{I}'_{(t+1)} \) based on the target viewpoint.
        </div>
      </div>
    </div>
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="./static/images/final_cvpr_fig3.png"
               class="interpolation-image"
               alt="Interpolate start reference image."
               style="height: 100%; width: 100%; object-fit: cover;"/>
        </div>
        <div class="content has-text-justified">
          Our framework aims to forecast a 3D scene into the future and view it from novel viewpoints. It comprises three primary steps:
        </div>
    
        <div class="content has-text-justified bold">
          <strong>1. Constructing 3D point cloud:</strong>
        </div>
    
        <div class="content has-text-justified">
          (1) Estimate the depth map \( \mathbf{D} \) from the input image \( \mathbf{I} \).
          <br>
          (2) Address "holes" in future frames caused by dis-occlusions from dynamic object motion:
          <br>
          (i) Segment dynamic category (foreground) objects to produce a binary mask \( \mathbf{M} \), identifying potential regions for "holes".
          <br>
          (ii) Mask these foreground regions in both input image and depth map, then inpaint them using the background context.
          <br>
          (3) Extract features from both original and inpainted frames to produce \( \mathcal{F}_{(t)} \) and \( \mathcal{F}^{\overline{\text{BG}}}_{(t)} \).
          <br>
          (4) Create 3D point cloud \( \mathcal{P} \) by unprojecting the 2D features \( \mathbf{F} \) and \( \mathbf{F}^{\overline{\text{BG}}} \) into 3D, using depth maps \( \mathbf{D} \) and \( \mathbf{D}^{\overline{\text{BG}}} \), respectively. For simplicity, we refer to the set \( \{\mathbf{F}, \mathbf{D}, \mathbf{M}\} \) as <em>original feature layer</em>, denoted by \( \mathcal{F} \), and the set \( \{\mathbf{F}^{\overline{\text{BG}}}, \mathbf{D}^{\overline{\text{BG}}}, \mathbf{M}\} \) as <em>inpainted feature layer</em> \( \mathcal{F}^{\overline{\text{BG}}} \).
        </div>
    
        <div class="content has-text-justified bold">
          <strong>2. Forecasting future 3D motion:</strong>
        </div>
    
        <div class="content has-text-justified">
          Given feature layers from past frames, our method forecasts future 3D motion flow in two stages:
          <br>
          (1) Ego-motion forecasting using the EMF module, which processes the background (static category) across frames using inpainted feature layers \( \mathcal{F}_{(t-1)}^{\overline{\text{BG}}} \) and \( \mathcal{F}_{(t)}^{\overline{\text{BG}}} \), yielding two relative ego-pose transformations, \( \mathcal{T}_{(t-1) \rightarrow (t+1)} \) and \( \mathcal{T}_{(t) \rightarrow (t+1)} \). These transformations lead to initial 3D motion flows \( \mathbf{u}^{0}_{(t-1) \rightarrow (t+1)} \) and \( \mathbf{u}^{0}_{(t) \rightarrow (t+1)} \), referred to as \( \mathbf{U}^{0}_{(t+1)} \).
          <br>
          (2) The OMF module then refines the initial 3D motion flow \( \mathbf{U}^{0}_{(t+1)} \) by accounting for foreground object motion, using original and inpainted feature layers to derive the final forecasted 3D motion flow, \( \mathbf{U}^{L}_{(t+1)} \), after \( L \) MMFB blocks.
        </div>
    
        <div class="content has-text-justified bold">
          <strong>3. Multi-scale motion flow block (MMFB):</strong>
        </div>
    
        <div class="content has-text-justified">
          We illustrate the design of an MMFB block here.
        </div>
      </div>
    </div>
    
    
        

        


        

     
    <!--/ Abstract. -->

    <!-- Paper video. -->
    
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Comparison on KITTI and Cityscapes</h2>
        <div class="content has-text-justified">
          <figure>
            <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/ECCV_9842_KITTI.mp4" type="video/mp4">
            </video>
            <figcaption>Comparison on KITTI dataset</figcaption>
          </figure>
        </div>
        
        <div class="content has-text-justified">
          <figure>
            <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/ECCV_Cityscapes.mp4" type="video/mp4">
            </video>
            <figcaption>Comparison on Cityscapes dataset</figcaption>
          </figure> 
        </div>
        
      </div>
    </div>
    
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Comparison on KITTI and Cityscapes</h2>
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <figure>
              <img src="./static/images/video_prediction_results.png"
                class="interpolation-image"
                alt="Interpolate start reference image."
                style="height: 100%; width: 100%; object-fit: cover;"/>
              <figcaption>Video Prediction Results on KITTI and Cityscapes</figcaption>
          </figure>
        </div>
        
        <div class="content has-text-justified">
          <figure style="display: inline-block; text-align: center; width: 70%;">
            <img src="./static/images/qual_NVS_results.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="width: 100%; height: auto; object-fit: cover;"/>
            <figcaption style="text-align: center; margin-top: 8px;">Comparison Results of Novel View Synthesis</figcaption>
          </figure> 
        </div>
        
        
      </div>
    </div>
    
  </div>
</section>

<!-- <h2 class="title is-3">Results</h2>
<section class="hero teaser" style="height: 100%; width: 100%;">
  <div class="container is-max-desktop" style="height: 100%; width: 100%;">

    <div class="hero-body" style="height: 100%; width: 100%; display: flex; justify-content: center; align-items: center;">
      
      <div class="column is-3 has-text-centered" style="height: 100%; width: 100%;">
        <img src="./static/videos/final_cvpr_fig2.png"
             class="interpolation-image"
             alt="Interpolate start reference image."
             style="height: 100%; width: 100%; object-fit: cover;"/>
             
        
      </div>
    </div>
  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yarram2024forecasting,
  author    = {Yarram, Sudhir and Yuan, Junsong},
  title     = {Forecasting Future Videos from Novel Views via Disentangled 3D Scene Representation},
  journal   = {ECCV},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <!-- <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
